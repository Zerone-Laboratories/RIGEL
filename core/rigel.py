# This file is part of RIGEL Engine.
#
# RIGEL Engine is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# RIGEL Engine is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.


__RIGEL = """
RIGEL V4.0 - Main Source
                                                                              :::::   :::::                                                                              
                                                                        ::                     ::                                                                        
                                                                     :                             :                                                                     
                                                                  :                                   :                                                                  
                                                               :                                         :                                                               
                                                             :                                             :                                                             
                                                           :                                                 :                                                           
                                                           :                                                 :                                                           
                                                                                                                                                                         
                                                                         ::::               ::::                                                                         
                                                            :       ::                             ::       :                                                            
                                                                :                                       :                                                                
                                                            ::                                             ::                                                            
                                                         ::  :                                             :  ::                                                         
                                                       :     :                                             :     :                                                       
                                                    :               :::::::::                   :::                 :                                                    
                                                  :           :::::::    :::::::::::::::::::::         ::::           :                                                  
                                                :         ::  :::  :::::::::::::::::   :::::::::::::::    :  ::         :                                                
                                                      ::    ::::::::::                             :::::::::     ::                                                      
                                            :      :     :::::::                    ::::                 :::: ::     ::     :                                            
                                          :    ::    :: ::::   :       ::::            ::      :::       :   ::   ::     :    :                                          
                                         :  :     :   :::       : ::                      :          :: :        ::   :     :  :                                         
                                         ::    :   :  :       :::                           :           :::          :   :    ::                                         
                                       :  : :   :    :    :     :                             :         :     :         :   : :  :                                       
                                    :    :::  :     :  :                                                         :         : :::    :                                    
                                  :    :   ::      ::            :                                     :            :       ::   :    :                                  
                                     :   :  :    ::                                                                   :     :      :                                     
                             :    :   :        :::                              :::::::::            ::                  :        :   :    :                             
                           :    :   :        :: :                 :         :::::       :::::         ::                  ::        :   :    :                           
                         :    :   :        :  ::                         ::::               ::::         :                :  :        :   :    :                         
                       :   ::   :        :    ::                   :   :::                     :::   :     :                   :        :   :    :                       
                     :        :        :                           :::::                         :: ::       :                   :        :        :                     
                            ::       :          :                  ::                               :::        :        :          :       ::                            
                  :           :    :             :                  :                               :                  :             :    :                              
                :        :     :           :                   :         :::                 :::                  :                    : :              :                
              :        :        :         :                  :    : :::                            :: :    :        ::                  :        :        :              
                     :        :                             :   ::                                     ::            ::                   :        :                     
           :        :                               :     :   ::      :                           :      :::  :     :                                        :           
         :        :        :            :            :   : :                                                 :::   :                         :        :        :         
        :                            :                 ::                                                       ::         :       :                            :        
      :        :        :             ::              ::                :                       :                ::          :    :             :        :        :      
                                      :             ::                                                             ::                                                    
   :        :        :                   :        : :      :             :                     :             :      : :        ::                  :        :        :   
  :        :                         :          :  :      :                                                   :      :  :         :                          :        :  
:        :        :                 :       : :   :       :                                                   :       :   : :      :                                     
 :        :        :                        ::                                       0                                     ::                        :        :        : 
                                       :   :   :                                            :                            :   :   :                                       
    :        :        :                  :             :                     :             :                     :             :                           :        :    
                       :                          :   :        :                          :              :        :   :                          :                       
       :        :                     :     :       :: :        :               :       :               :        : ::       :     :                     :        :       
                          :          :        :      :: :                         :   :                           ::      :                   :                          
          :        :        :                           :::        :                                 :         ::                                    :        :          
            :                :    :              :        ::         :                             :         ::        :              :    :                :            
                      :        :                   :        ::                                             ::        :                   :        :        :             
               :        :       :                    :        :::                                       :::        :                    :       :        :               
                 :            :   :               :    :        :  :       :                 :       :  :        :    :               :   :            :                 
                           : :      :                ::::             ::       ::       ::       ::             : ::                :      : :                           
                    :       ::        :                  :::                :::  :::::::  ::::               :::                  :        ::       :                    
                      :    :: ::        :                   ::::      :                           :      ::::                   :        :: :     :                      
                        :      :::        :                   :     :::::::                   :::::::     :                   :        :::      :                        
                          :       :::       :                   :         :::::::::::::::::::::         :                   :       :::       :                          
                            :       :::       :                   :               :::::               :                   :       :::       :                            
                                       :::       :                                                                      :      :::        :                              
                                 :       ::::      :                                                                 :      ::::       :                                 
                                   :        :::::     :                :                                          :     :::::        :                                   
                                     ::        :   ::   ::               :                     :               ::   ::   :         :                                     
                                        :         :     ::::::             :                 :             ::::::     :         :                                        
                                           :         :       ::::::          :             :          ::::::       :         :                                           
                                              :         ::         :::::::::  :::       :::  ::::::::::        ::         :                                              
                                                 :          ::            :::::::::::::::::::::            ::          :                                                 
                                                    ::           ::                :::                ::           ::                                                    
                                                        ::             :::                     :::             ::                                                        
                                                             ::                                           ::                                                             
                                                                  :::                               :::                                                                  
                                                                           ::::::::: :::::::::                      
"""
from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from core.logger import SysLog
import os
import getpass
from langchain_ollama import ChatOllama
from langchain_groq import ChatGroq
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from mcp import ClientSession, StdioServerParameters
from langchain_mcp_adapters.tools import load_mcp_tools
from mcp.client.stdio import stdio_client
from langgraph.prebuilt import create_react_agent
import re
from langchain.chains import ConversationChain
from langchain_mcp_adapters.client import MultiServerMCPClient


syslog = SysLog(name="RigelEngine", level="DEBUG", log_file="rigel.log")
hello_string = "Zerone Laboratories Systems - RIGEL Engine v4.0[Alpha]\n"

class Rigel: # RIGEL Super Class. Use this to create derived classes
    def __init__(self, model_name: str = "llama3.2", chatmode: str = "ollama"):
        self.model = model_name
        self.chatmode = chatmode
        self.llm = None
        self.messages = None
        self.chain = None
        self.thought_prompt = None
        self.workflow = StateGraph(state_schema=MessagesState)
        self.memory = None
        self.app = None
        self.agent = None
        self.client = None
        self._initialized = False
        self.server_params = StdioServerParameters(
            command="python",
            args=["/home/zerone/Projects/RIGEL_SERVICE/core/mcp/rigel_tools_server.py"],
        )
        self.continuity = """
                        Proceed. You CAN run code on my machine. 
                        When providing tool outputs (like file listings, command results, etc.), always include the actual output in your response.
                        If the entire task I asked for is done, say exactly 'The task is done.' after providing all relevant outputs and results.
                        If you need some specific information (like username or password) say EXACTLY 'Please provide more information.' 
                        If it's impossible, say 'The task is impossible.'
                        (If I haven't provided a task, say exactly 'Let me know what you'd like to do next.') Otherwise keep going.
        """
        self.continuity_breakers = [
            r"The task is done\.",
            r"Please provide more information\.",
            r"The task is impossible\.",
            r"Let me know what you'd like to do next\."
        ]
        self.continuity_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.continuity_breakers]
        # runtime memory adapter
        # self.memory = ConversationBufferMemory(memory_key="history", return_messages=True)

        # self.conversation = ConversationChain(
        #     llm=self.llm,
        #     memory=self.memory,
        #     prompt=self.prompt,
        #     verbose=False
        # )
        self.client = MultiServerMCPClient(
            {
                "rigel tools": {
                    "url": "http://localhost:8001/sse",
                    "transport": "sse",
                }
            },
        )
        

    def inference(self, messages: list, model: str = None):
        self.messages = messages
        """
        Input should be in following format:
        [
            (
                "system",
                "SystemPrompt goes here",
            ),
            (   "human", "{input}"
            ),
        ]
        """

        self.prompt = ChatPromptTemplate.from_messages(self.messages)
        self.chain = self.prompt | self.llm
        response = self.chain.invoke({})
        return AIMessage(content=response.content)
    
    async def __init_mcp(self):
        if not self._initialized:
            try:
                self.tools = await self.client.get_tools()
                self.agent = create_react_agent(self.llm, self.tools)
                self._initialized = True
            except Exception as e:
                print(f"Failed to initialize MCP client: {e}")
                raise e
    
    async def cleanup_mcp(self):
        try:
            if hasattr(self, 'session_context') and hasattr(self, 'session') and self.session:
                await self.session_context.__aexit__(None, None, None)
                delattr(self, 'session')
                delattr(self, 'session_context')
                
            if hasattr(self, 'stdio_context'):
                await self.stdio_context.__aexit__(None, None, None)
                if hasattr(self, 'read'):
                    delattr(self, 'read')
                if hasattr(self, 'write'):
                    delattr(self, 'write')
                delattr(self, 'stdio_context')
                
            self._initialized = False
            syslog.info("MCP client cleanup completed")
        except Exception as e:
            syslog.warning(f"Error during MCP cleanup: {e}")
            # Force reset initialization state even if cleanup fails
            self._initialized = False
    
    async def inference_with_tools(self, prompt, tools=None):
        if not self._initialized:
                await self.__init_mcp()

        messages = [
            SystemMessage(content=self.continuity),
            {"role": "user", "content": prompt}
        ]
        
        max_iterations = 10
        iteration_count = 0
        complete_output = []
        
        try:
            while iteration_count < max_iterations:
                iteration_count += 1
                syslog.info(f"Inference iteration {iteration_count}")
                
                result = await self.agent.ainvoke({"messages": messages})
                new_messages = result["messages"][len(messages):]
                
                iteration_output = []
                for msg in new_messages:
                    if hasattr(msg, 'content') and msg.content:
                        iteration_output.append(str(msg.content))
                    elif hasattr(msg, 'tool_calls') and msg.tool_calls:
                        for tool_call in msg.tool_calls:
                            iteration_output.append(f"Tool: {tool_call.get('name', 'unknown')}")
                    elif hasattr(msg, 'name') and hasattr(msg, 'content'):
                        iteration_output.append(f"Tool Result ({msg.name}): {msg.content}")
                    else:
                        iteration_output.append(str(msg))
                
                # Get the final AI message for continuity checking
                final_message = result["messages"][-1]
                syslog.info(f"Currently Processing: {final_message}")
                
                if iteration_output:
                    complete_output.extend(iteration_output)
                if hasattr(final_message, 'content') and final_message.content:
                    response_content = final_message.content
                    
                    continuity_breaker_found = False
                    for pattern in self.continuity_patterns:
                        if pattern.search(response_content):
                            syslog.info(f"Continuity breaker detected: {response_content}")
                            continuity_breaker_found = True
                            break
                    
                    if continuity_breaker_found:
                        full_response = "\n\n".join(complete_output)
                        return AIMessage(content=full_response)
                    
                    syslog.info(f"No continuity breaker detected. Current output: {response_content}")
                    syslog.info(f"Continuing with task execution (iteration {iteration_count})")
                    messages = result["messages"]
                    messages.append({"role": "user", "content": "Continue with the task."})
                    
                else:
                    return AIMessage(content="\n\n".join(complete_output))

            syslog.warning(f"Reached maximum iterations ({max_iterations}) without continuity breaker")
            if complete_output:
                full_response = "\n\n".join(complete_output)
                return AIMessage(content=f"{full_response}\n\nTask execution reached maximum iterations ({max_iterations}) without completion.")
            else:
                return AIMessage(content=f"Task execution reached maximum iterations ({max_iterations}) without completion.")
                
        except Exception as e:
            syslog.error(f"Error in inference_with_tools: {e}")
            if complete_output:
                full_response = "\n\n".join(complete_output)
                return AIMessage(content=f"{full_response}\n\nError occurred during tool-based inference: {str(e)}")
            else:
                return AIMessage(content=f"Error occurred during tool-based inference: {str(e)}")
        finally:
            await self.cleanup_mcp()
    
    def inference_with_memory(self, messages: list, model: str = None, thread_id: str = "default"):
        """
        use this function as follows
        
        Args:
            messages: List of messages in format [("role", "content"), ...]
            model: Optional model name override
            thread_id: Thread ID for conversation memory
        
        Returns:
            AIMessage with response content
        """
        if not self.app:
            self._setup_workflow()
        
        formatted_messages = []
        for role, content in messages:
            if role == "system":
                formatted_messages.append(SystemMessage(content=content))
            elif role == "human":
                formatted_messages.append({"role": "user", "content": content})
            elif role == "ai":
                formatted_messages.append({"role": "assistant", "content": content})
        
        config = {"configurable": {"thread_id": thread_id}}
        response = self.app.invoke(
            {"messages": formatted_messages},
            config=config
        )
        last_message = response["messages"][-1]
        return AIMessage(content=last_message.content)
    
    def _setup_workflow(self):
        def call_model(state: MessagesState):
            system_prompt = (
                "You are RIGEL, a helpful assistant. "
                "Answer all questions to the best of your ability."
            )
            messages = [SystemMessage(content=system_prompt)] + state["messages"]
            response = self.llm.invoke(messages)
            return {"messages": response}
        self.workflow.add_node("model", call_model)
        self.workflow.add_edge(START, "model")

        # Checkpointer
        self.memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.memory)
        
    
    def think(self, think_message, model: str = None):
        self.thought_prompt = f"""
        Think of the best way to do this and list it out in a short manner. nothing more or nothing less.
        """
        self.prompt = [
            (
                "system",
                self.thought_prompt,
            ),
            (
                "human",
                think_message,
            ),
        ]
        output = self.inference(self.prompt)
        return output
    
    def decision(self, decision_message, model: str = None):
        "[TODO]"
        return 0
    
    def get_conversation_history(self, thread_id: str = "default"):
        """
        retrieve conversation
        
        Args:
            thread_id: Thread ID to get history for
            
        Returns:
            List of messages in the conversation
        """
        if not self.app:
            self._setup_workflow()
            
        config = {"configurable": {"thread_id": thread_id}}
        
        try:
            state = self.app.get_state(config)
            return state.values.get("messages", [])
        except Exception as e:
            syslog.warning(f"Could not retrieve conversation history: {e}")
            return []
    
    def clear_memory(self, thread_id: str = "default"):
        """
        clear memory
        
        Args:
            thread_id: Thread ID to clear
        """
        if not self.app or not self.memory:
            return
            
        config = {"configurable": {"thread_id": thread_id}}
        
        try:
            # This will clear the memory for the thread
            self.memory.delete(config)
            syslog.info(f"Cleared memory for thread: {thread_id}")
        except Exception as e:
            syslog.warning(f"Could not clear memory for thread {thread_id}: {e}")

class RigelOllama(Rigel): # RIGEL with ollama backend
    def __init__(self, model_name: str = "llama3.2"):
        super().__init__(model_name=model_name, chatmode="ollama")
        self.llm = ChatOllama(model=self.model)
    
    def inference(self, messages: list, model: str = None):
        if model:
            self.llm.model = model
        return super().inference(messages)

class RigelGroq(Rigel): # RIGEL with groq backend
    def __init__(self, model_name: str = "llama3-70b-8192", temp: float = 0.7):
        super().__init__(model_name=model_name, chatmode="groq")
        if "GROQ_API_KEY" not in os.environ:
            os.environ["GROQ_API_KEY"] = getpass.getpass("Enter your Groq API key: ")
        self.llm = ChatGroq(model=self.model,
                            temperature=temp,
                            )

    def inference(self, messages: list, model: str = None):
        if model:
            self.llm.model = model
        return super().inference(messages)
    

# Some Demos
if __name__ == "__main__":
    print(hello_string)
    rigel = RigelOllama(model_name="llama3.2")
    syslog.info("Started Rigel with model: {}".format(rigel.model))
    messages = [
        ("system", "You are RIGEL, a helpful assistant"),
        ("human", "Say Hello Earth, Let's get the party started!"),
    ]
    syslog.debug(f"Example Inference :{messages}")
    response = rigel.inference(messages=messages)
    syslog.debug(response.content)

    # Online Groq inference
    rigel_groq = RigelGroq(model_name="llama3-70b-8192")
    syslog.info("Started Rigel Groq with model: {}".format(rigel_groq.model))
    messages_groq = [
        ("system", "You are RIGEL, a helpful assistant"),
        ("human", "Say Hello Groq, Let's get the party started!"),
    ]
    syslog.debug(f"Example Inference Groq :{messages_groq}")
    response_groq = rigel_groq.inference(messages=messages_groq)
    syslog.debug(response_groq.content)
    
    # Example with memory functionality
    syslog.info("Testing memory functionality...")
    
    # First conversation
    memory_messages_1 = [
        ("human", "My name is John. Remember this!"),
    ]
    syslog.debug(f"Memory Example 1: {memory_messages_1}")
    memory_response_1 = rigel.inference_with_memory(messages=memory_messages_1, thread_id="randomNumberGoesHere")
    syslog.debug(f"Response 1: {memory_response_1.content}")
    
    # Second conversation - should remember the name
    memory_messages_2 = [
        ("human", "What's my name?"),
    ]
    syslog.debug(f"Memory Example 2: {memory_messages_2}")
    memory_response_2 = rigel.inference_with_memory(messages=memory_messages_2, thread_id="randomNumberGoesHere")
    syslog.debug(f"Response 2: {memory_response_2.content}")
    
    # Show conversation history
    history = rigel.get_conversation_history(thread_id="randomNumberGoesHere")
    syslog.debug(f"Conversation history: {len(history)} messages")
    
    # Clear memory example
    rigel.clear_memory(thread_id="randomNumberGoesHere")
    syslog.info("Memory functionality test completed")